{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69eee883-00c2-4d64-9ead-8af179e1b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "## 1. Setup and Imports (Following Lab Solution Structure)\n",
    "## ----------------------------------------------------------------------\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Note: Other imports (math, numpy, collections, array, json, re, time) assumed present\n",
    "from collections import defaultdict\n",
    "from array import array \n",
    "import re \n",
    "\n",
    "# --- NLTK Setup (Necessary for Point 1) ---\n",
    "try:\n",
    "    # Check if stopwords are already downloaded\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading nltk stopwords...\")\n",
    "    # NOTE: In a complete notebook, this would be uncommented: nltk.download('stopwords')\n",
    "\n",
    "# Placeholder for DOCS variable after successful loading (28080 docs)\n",
    "# DOCS = [...] \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 2. Enhanced Pre-processing Function (Point 1)\n",
    "## ----------------------------------------------------------------------\n",
    "\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess text: lowercasing, punctuation removal, tokenization,\n",
    "    stop word removal, stemming, and filtering (bonus).\n",
    "    \"\"\"\n",
    "    # Uses PorterStemmer for consistency with the provided lab solution.\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # 1. Lowercase\n",
    "    line = line.lower()\n",
    "\n",
    "    # 2. Removing punctuation marks (Required by Point 1)\n",
    "    line = re.sub(r'[^\\w\\s]', ' ', line)\n",
    "\n",
    "    # 3. Tokenization\n",
    "    line = line.split()\n",
    "\n",
    "    # 4. Removing stop words\n",
    "    line = [x for x in line if x not in stop_words]\n",
    "\n",
    "    # 5. Stemming (Required by Point 1)\n",
    "    line = [stemmer.stem(word) for word in line]\n",
    "\n",
    "    # BONUS: Filtering single-character/pure digits (Point 1: \"anything else you think it's needed\")\n",
    "    line = [x for x in line if len(x) > 1 and not x.isdigit()]\n",
    "\n",
    "    return line\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 3. Helper Function for Complex Fields (product_details)\n",
    "## ----------------------------------------------------------------------\n",
    "\n",
    "def get_product_details_string(product_details_list):\n",
    "    \"\"\"Flattens the list of product_details dictionaries into a single string for indexing.\"\"\"\n",
    "    detail_strings = []\n",
    "    if isinstance(product_details_list, list):\n",
    "        for detail_dict in product_details_list:\n",
    "            if isinstance(detail_dict, dict):\n",
    "                # Concatenate keys and values\n",
    "                for key, value in detail_dict.items():\n",
    "                    # We only care about the values in the details, e.g., 'Cotton Blend'\n",
    "                    detail_strings.append(str(value)) \n",
    "    return \" \".join(detail_strings)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 4. Integrated Index Creation Function (Points 1, 2, 3, & 4 Combined)\n",
    "## ----------------------------------------------------------------------\n",
    "\n",
    "def create_multi_field_index(DOCS):\n",
    "    \"\"\"\n",
    "    Builds the multi-field inverted index, satisfying all Part 1 points.\n",
    "    \"\"\"\n",
    "    # Core Inverted Index structures (Point 3: Multi-Field Indexing)\n",
    "    text_index = defaultdict(list)    \n",
    "    category_index = defaultdict(list) \n",
    "\n",
    "    # Document Store for Full Retrieval (Point 2 and Hint 3: PID preservation)\n",
    "    doc_store = {} \n",
    "    \n",
    "    # We also need structures to compute TF-IDF (DF is needed now, TF and IDF will be completed later)\n",
    "    # The DF is required now to check how many documents contain a term across the corpus.\n",
    "    df_text = defaultdict(int)\n",
    "    df_category = defaultdict(int)\n",
    "\n",
    "    for doc in DOCS:\n",
    "        pid = doc.get('pid')\n",
    "        if not pid:\n",
    "            continue\n",
    "\n",
    "        # --- Point 2: Document Store (PID Mapping) ---\n",
    "        # Store the full document using its unique PID. PID is used for evaluation (Hint 3).\n",
    "        doc_store[pid] = doc\n",
    "\n",
    "        # --- Point 3: Strategic Field Combination for Indexing ---\n",
    "\n",
    "        # 1. Main Text for TEXT_INDEX (Content & Description)\n",
    "        prod_details_str = get_product_details_string(doc.get('product_details', []))\n",
    "        main_text = (\n",
    "            doc.get('title', '') + \" \" +          \n",
    "            doc.get('description', '') + \" \" +    \n",
    "            prod_details_str                      \n",
    "        )\n",
    "        \n",
    "        # 2. Categorical Text for CATEGORY_INDEX (Attributes & Brand)\n",
    "        # These terms will be boosted in ranking due to their high relevance to queries (Hint 1).\n",
    "        category_text = (\n",
    "            doc.get('brand', '') + \" \" +         \n",
    "            doc.get('category', '') + \" \" +\n",
    "            doc.get('sub_category', '')\n",
    "            # We omit 'seller' as it's typically less relevant for product content queries like the hints.\n",
    "        )\n",
    "        \n",
    "        # --- Point 4: Numeric/Status Fields ---\n",
    "        # Fields like 'out_of_stock', 'selling_price', 'average_rating' are NOT indexed as terms.\n",
    "        # Their raw values are in the DOC_STORE (Point 2) and will be used for filtering/boosting during ranking.\n",
    "\n",
    "        # --- Point 1/3: Process and Build Indexes ---\n",
    "        \n",
    "        # 1. Index Main Text\n",
    "        terms_main_text = build_terms(main_text)\n",
    "        current_page_index_text = {}\n",
    "        for position, term in enumerate(terms_main_text):\n",
    "            if term not in current_page_index_text:\n",
    "                current_page_index_text[term] = [pid, array('I', [])] \n",
    "            current_page_index_text[term][1].append(position)\n",
    "        \n",
    "        # Merge and update DF for main text\n",
    "        for term, posting in current_page_index_text.items():\n",
    "            text_index[term].append(posting)\n",
    "            df_text[term] += 1 # Update document frequency\n",
    "\n",
    "        # 2. Index Categorical Text\n",
    "        terms_category = build_terms(category_text)\n",
    "        current_page_index_category = {}\n",
    "        for position, term in enumerate(terms_category):\n",
    "            if term not in current_page_index_category:\n",
    "                current_page_index_category[term] = [pid, array('I', [])]\n",
    "            current_page_index_category[term][1].append(position)\n",
    "\n",
    "        # Merge and update DF for categorical text\n",
    "        for term, posting in current_page_index_category.items():\n",
    "            category_index[term].append(posting)\n",
    "            df_category[term] += 1 # Update document frequency\n",
    "\n",
    "\n",
    "    print(f\"Indexing complete. Created Indexing Structures:\")\n",
    "    print(f\"- Documents stored for full retrieval: {len(doc_store)}\")\n",
    "    print(f\"- Main Text Index unique terms: {len(text_index)}\")\n",
    "    print(f\"- Category Index unique terms: {len(category_index)}\")\n",
    "    \n",
    "    # Return the indexes and DF (required for subsequent TF/IDF calculation)\n",
    "    return text_index, category_index, doc_store, df_text, df_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14debad9-9f7d-464b-9f4d-c4f95f375a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
